import { Request, Response } from "express";

const sleep = (ms: number) => new Promise((r) => setTimeout(r, ms));

export async function streamLab(req: Request, res: Response) {
  res.setHeader("Content-Type", "text/plain; charset=utf-8");
  res.setHeader("Cache-Control", "no-cache");
  res.setHeader("Connection", "keep-alive");

  res.flushHeaders();

  res.write(
    "Уявімо собі систему, яка поступово накопичує знання, але не у вигляді чітко структурованої бази даних, а у вигляді фрагментів досвіду, текстів, спостережень, пояснень і напівформалізованих думок. Така система дуже схожа на людську пам’ять: частина інформації є чіткою й формалізованою, частина — розмитою, частина — контекстною, а частина взагалі має сенс лише у зв’язку з іншими фрагментами. Саме з такими даними найгірше працюють класичні підходи, зате добре справляються сучасні мовні моделі та векторні представлення. Коли ми говоримо про великі текстові чанки, важливо розуміти, що їхній розмір сам по собі не є ані добром, ані злом. Великий chunk може містити достатньо контексту, щоб модель зрозуміла причинно-наслідкові зв’язки, але водночас він може містити надлишкову інформацію, яка заважає точному пошуку. У RAG-системах це проявляється дуже чітко: якщо chunk надто великий, embedding «розмазується», і релевантний сигнал губиться серед другорядних деталей. Проте у тестових цілях штучно створений великий chunk є надзвичайно корисним. Він дозволяє перевірити, як система поводиться з довгими пасажами тексту, чи коректно відбувається розбиття на токени, як працює стримінг, чи не виникають проблеми з лімітами контексту, і чи стабільно модель зберігає когерентність відповіді. У реальних умовах такі чанки можуть виникати, наприклад, при завантаженні документації, технічних специфікацій, юридичних текстів або транскриптів інтерв’ю. Ще один аспект — семантична щільність. Один і той самий обсяг тексту може бути або надзвичайно насиченим сенсом, або майже порожнім. У першому випадку кожне речення несе нову інформацію, у другому — текст лише повторює ті самі думки різними словами. Для embeddings це має велике значення, адже модель намагається усереднити семантику всього chunk’а. Саме тому часто рекомендують робити chunks не просто за розміром, а за змістовими межами — абзацами, підтемами або логічними блоками. Водночас у навчальних або експериментальних сценаріях інколи навмисно створюють «погані» чанки — надто довгі, неоднорідні, з кількома темами одразу. Це дозволяє побачити межі системи, зрозуміти, де вона починає помилятися, і які евристики реально працюють, а які — ні. Наприклад, можна порівняти відповіді моделі, коли їй подають один великий chunk проти кількох менших, але тематично цілісних. Цікавий момент полягає в тому, що для людини великий шматок тексту часто є проблемою — він втомлює, складно швидко знайти потрібне місце, важко тримати увагу. Для моделі ж це просто послідовність токенів, і проблема виникає лише тоді, коли контекст перевищує допустимий ліміт або коли інформація суперечлива. У цьому сенсі моделі парадоксально «терплячіші» за людей, але водночас менш вибіркові. Якщо перенести це в практичну площину, наприклад у розробку додатка або сервісу, то штучні великі чанки можна використовувати для навантажувального тестування. Чи не падає система при обробці довгого тексту? Чи стабільно працює пошук? Чи не зростає латентність до неприйнятного рівня? Усі ці питання краще з’ясувати на етапі експериментів, ніж уже в продакшені. Окремо варто згадати про мовну різноманітність. Текст українською, англійською чи німецькою може мати різну середню довжину слів, різну кількість токенів на одне й те саме речення, і це впливає на реальний розмір chunk’а в токенах, навіть якщо в байтах або символах він виглядає однаково. Тому тестові чанки бажано робити саме тією мовою, якою система реально працюватиме. У підсумку, великий штучний chunk — це не просто «багато тексту». Це інструмент. Інструмент для перевірки гіпотез, для виявлення слабких місць, для кращого розуміння того, як мовна модель сприймає і обробляє інформацію. І хоча в реальному застосуванні часто прагнуть до більш дрібних і акуратних блоків, уміння працювати з великими, неідеальними шматками тексту є важливою частиною інженерної зрілості системи. Цей текст навмисно не має чіткої структури, заголовків або списків. Він є безперервним потоком думок, який імітує реальний «сирий» контент: документацію, нотатки, пояснення або технічний щоденник. Саме такі дані найчастіше потрапляють у системи зберігання знань і стають матеріалом для подальшої обробки. Якщо система коректно працює з такими чанками, значить вона готова і до більш акуратних, структурованих сценаріїв.Уявімо собі систему, яка поступово накопичує знання, але не у вигляді чітко структурованої бази даних, а у вигляді фрагментів досвіду, текстів, спостережень, пояснень і напівформалізованих думок. Така система дуже схожа на людську пам’ять: частина інформації є чіткою й формалізованою, частина — розмитою, частина — контекстною, а частина взагалі має сенс лише у зв’язку з іншими фрагментами. Саме з такими даними найгірше працюють класичні підходи, зате добре справляються сучасні мовні моделі та векторні представлення. Коли ми говоримо про великі текстові чанки, важливо розуміти, що їхній розмір сам по собі не є ані добром, ані злом. Великий chunk може містити достатньо контексту, щоб модель зрозуміла причинно-наслідкові зв’язки, але водночас він може містити надлишкову інформацію, яка заважає точному пошуку. У RAG-системах це проявляється дуже чітко: якщо chunk надто великий, embedding «розмазується», і релевантний сигнал губиться серед другорядних деталей. Проте у тестових цілях штучно створений великий chunk є надзвичайно корисним. Він дозволяє перевірити, як система поводиться з довгими пасажами тексту, чи коректно відбувається розбиття на токени, як працює стримінг, чи не виникають проблеми з лімітами контексту, і чи стабільно модель зберігає когерентність відповіді. У реальних умовах такі чанки можуть виникати, наприклад, при завантаженні документації, технічних специфікацій, юридичних текстів або транскриптів інтерв’ю. Ще один аспект — семантична щільність. Один і той самий обсяг тексту може бути або надзвичайно насиченим сенсом, або майже порожнім. У першому випадку кожне речення несе нову інформацію, у другому — текст лише повторює ті самі думки різними словами. Для embeddings це має велике значення, адже модель намагається усереднити семантику всього chunk’а. Саме тому часто рекомендують робити chunks не просто за розміром, а за змістовими межами — абзацами, підтемами або логічними блоками. Водночас у навчальних або експериментальних сценаріях інколи навмисно створюють «погані» чанки — надто довгі, неоднорідні, з кількома темами одразу. Це дозволяє побачити межі системи, зрозуміти, де вона починає помилятися, і які евристики реально працюють, а які — ні. Наприклад, можна порівняти відповіді моделі, коли їй подають один великий chunk проти кількох менших, але тематично цілісних. Цікавий момент полягає в тому, що для людини великий шматок тексту часто є проблемою — він втомлює, складно швидко знайти потрібне місце, важко тримати увагу. Для моделі ж це просто послідовність токенів, і проблема виникає лише тоді, коли контекст перевищує допустимий ліміт або коли інформація суперечлива. У цьому сенсі моделі парадоксально «терплячіші» за людей, але водночас менш вибіркові. Якщо перенести це в практичну площину, наприклад у розробку додатка або сервісу, то штучні великі чанки можна використовувати для навантажувального тестування. Чи не падає система при обробці довгого тексту? Чи стабільно працює пошук? Чи не зростає латентність до неприйнятного рівня? Усі ці питання краще з’ясувати на етапі експериментів, ніж уже в продакшені. Окремо варто згадати про мовну різноманітність. Текст українською, англійською чи німецькою може мати різну середню довжину слів, різну кількість токенів на одне й те саме речення, і це впливає на реальний розмір chunk’а в токенах, навіть якщо в байтах або символах він виглядає однаково. Тому тестові чанки бажано робити саме тією мовою, якою система реально працюватиме. У підсумку, великий штучний chunk — це не просто «багато тексту». Це інструмент. Інструмент для перевірки гіпотез, для виявлення слабких місць, для кращого розуміння того, як мовна модель сприймає і обробляє інформацію. І хоча в реальному застосуванні часто прагнуть до більш дрібних і акуратних блоків, уміння працювати з великими, неідеальними шматками тексту є важливою частиною інженерної зрілості системи. Цей текст навмисно не має чіткої структури, заголовків або списків. Він є безперервним потоком думок, який імітує реальний «сирий» контент: документацію, нотатки, пояснення або технічний щоденник. Саме такі дані найчастіше потрапляють у системи зберігання знань і стають матеріалом для подальшої обробки. Якщо система коректно працює з такими чанками, значить вона готова і до більш акуратних, структурованих сценаріїв.",
  );
  await sleep(200);

  res.write("lo ");
  await sleep(200);

  res.write("wor");
  await sleep(200);

  res.write("ld");

  res.end();
}
